{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e09d6ac-6777-4db0-8bb6-0846055e3f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "97976603-ede0-4c22-868e-86ed127457a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in the dataset with pandas\n",
    "data = pd.read_csv(\"banana_quality.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e64436f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/15/txb5fw214h17glq1xh1l0h3c0000gn/T/ipykernel_29252/2323477838.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = pd.DataFrame(data).dropna().replace({\"Good\": 0, \"Bad\": 1})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Size</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Sweetness</th>\n",
       "      <th>Softness</th>\n",
       "      <th>HarvestTime</th>\n",
       "      <th>Ripeness</th>\n",
       "      <th>Acidity</th>\n",
       "      <th>Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.924968</td>\n",
       "      <td>0.468078</td>\n",
       "      <td>3.077832</td>\n",
       "      <td>-1.472177</td>\n",
       "      <td>0.294799</td>\n",
       "      <td>2.435570</td>\n",
       "      <td>0.271290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.409751</td>\n",
       "      <td>0.486870</td>\n",
       "      <td>0.346921</td>\n",
       "      <td>-2.495099</td>\n",
       "      <td>-0.892213</td>\n",
       "      <td>2.067549</td>\n",
       "      <td>0.307325</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.357607</td>\n",
       "      <td>1.483176</td>\n",
       "      <td>1.568452</td>\n",
       "      <td>-2.645145</td>\n",
       "      <td>-0.647267</td>\n",
       "      <td>3.090643</td>\n",
       "      <td>1.427322</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.868524</td>\n",
       "      <td>1.566201</td>\n",
       "      <td>1.889605</td>\n",
       "      <td>-1.273761</td>\n",
       "      <td>-1.006278</td>\n",
       "      <td>1.873001</td>\n",
       "      <td>0.477862</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.651825</td>\n",
       "      <td>1.319199</td>\n",
       "      <td>-0.022459</td>\n",
       "      <td>-1.209709</td>\n",
       "      <td>-1.430692</td>\n",
       "      <td>1.078345</td>\n",
       "      <td>2.812442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>-6.414403</td>\n",
       "      <td>0.723565</td>\n",
       "      <td>1.134953</td>\n",
       "      <td>2.952763</td>\n",
       "      <td>0.297928</td>\n",
       "      <td>-0.156946</td>\n",
       "      <td>2.398091</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>0.851143</td>\n",
       "      <td>-2.217875</td>\n",
       "      <td>-2.812175</td>\n",
       "      <td>0.489249</td>\n",
       "      <td>-1.323410</td>\n",
       "      <td>-2.316883</td>\n",
       "      <td>2.113136</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>1.422722</td>\n",
       "      <td>-1.907665</td>\n",
       "      <td>-2.532364</td>\n",
       "      <td>0.964976</td>\n",
       "      <td>-0.562375</td>\n",
       "      <td>-1.834765</td>\n",
       "      <td>0.697361</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>-2.131904</td>\n",
       "      <td>-2.742600</td>\n",
       "      <td>-1.008029</td>\n",
       "      <td>2.126946</td>\n",
       "      <td>-0.802632</td>\n",
       "      <td>-3.580266</td>\n",
       "      <td>0.423569</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>-2.660879</td>\n",
       "      <td>-2.044666</td>\n",
       "      <td>0.159026</td>\n",
       "      <td>1.499706</td>\n",
       "      <td>-1.581856</td>\n",
       "      <td>-1.605859</td>\n",
       "      <td>1.435644</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Size    Weight  Sweetness  Softness  HarvestTime  Ripeness  \\\n",
       "0    -1.924968  0.468078   3.077832 -1.472177     0.294799  2.435570   \n",
       "1    -2.409751  0.486870   0.346921 -2.495099    -0.892213  2.067549   \n",
       "2    -0.357607  1.483176   1.568452 -2.645145    -0.647267  3.090643   \n",
       "3    -0.868524  1.566201   1.889605 -1.273761    -1.006278  1.873001   \n",
       "4     0.651825  1.319199  -0.022459 -1.209709    -1.430692  1.078345   \n",
       "...        ...       ...        ...       ...          ...       ...   \n",
       "7995 -6.414403  0.723565   1.134953  2.952763     0.297928 -0.156946   \n",
       "7996  0.851143 -2.217875  -2.812175  0.489249    -1.323410 -2.316883   \n",
       "7997  1.422722 -1.907665  -2.532364  0.964976    -0.562375 -1.834765   \n",
       "7998 -2.131904 -2.742600  -1.008029  2.126946    -0.802632 -3.580266   \n",
       "7999 -2.660879 -2.044666   0.159026  1.499706    -1.581856 -1.605859   \n",
       "\n",
       "       Acidity  Quality  \n",
       "0     0.271290        0  \n",
       "1     0.307325        0  \n",
       "2     1.427322        0  \n",
       "3     0.477862        0  \n",
       "4     2.812442        0  \n",
       "...        ...      ...  \n",
       "7995  2.398091        1  \n",
       "7996  2.113136        1  \n",
       "7997  0.697361        1  \n",
       "7998  0.423569        1  \n",
       "7999  1.435644        1  \n",
       "\n",
       "[8000 rows x 8 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking out the different features\n",
    "df = pd.DataFrame(data).dropna().replace({\"Good\": 0, \"Bad\": 1})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "49ff1429",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Deviding the dataset into an X-matrix and an y-matrix\n",
    "X = df.drop(columns=['Quality'])\n",
    "y = df['Quality']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e3f20fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6400)\n",
      "(7, 6400)\n",
      "\n",
      "(1, 1600)\n",
      "(7, 1600)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the dataset intto training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n",
    "\n",
    "\n",
    "X_test = X_test.T\n",
    "y_test = np.reshape(y_test, (1, 6400))\n",
    "y_test= np.where(y_test == \"good\", 1, 0)\n",
    "print(y_test.shape)\n",
    "print(X_test.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "X_train = X_train.T\n",
    "y_train = np.reshape(y_train, (1, 1600))\n",
    "y_train= np.where(y_train == \"good\", 1, 0)\n",
    "\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "976e6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing parameters\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "\n",
    "  parameters = {}\n",
    "\n",
    "  L = len(layer_dims)\n",
    "\n",
    "  for l in range(1, L):\n",
    "\n",
    "    parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "    parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "\n",
    "  return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "44caa456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear part of each neuron\n",
    "\n",
    "def linear_part(A, W, b):\n",
    "\n",
    "  Z = np.dot(W,A) + b\n",
    "  cache = (A, W, b)\n",
    "\n",
    "\n",
    "\n",
    "  return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "81b5fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid function\n",
    "def sigmoid(Z):\n",
    "  return 1/(1+np.exp(-Z)), Z\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "04953b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relu function\n",
    "def relu(Z):\n",
    "\n",
    "  return np.maximum(0,Z), Z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b7e4cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation part of each neuron\n",
    "def activation_part(A_prev, W, b, activation):\n",
    "\n",
    "  if activation == \"sigmoid\":\n",
    "    Z, linear_cache = linear_part(A_prev, W, b)\n",
    "    A, activation_cache = sigmoid(Z)\n",
    "\n",
    "\n",
    "\n",
    "  elif activation == \"relu\":\n",
    "    Z, linear_cache = linear_part(A_prev, W, b)\n",
    "    A, activation_cache = relu(Z)\n",
    "\n",
    "  cache = (linear_cache, activation_cache)\n",
    "\n",
    "  return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a45fb311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The forward propagation\n",
    "def L_model_forward(X, parameters):\n",
    "\n",
    "  caches = []\n",
    "  A = X\n",
    "  L = len(parameters) // 2\n",
    "\n",
    "\n",
    "  for l in range(1, L):\n",
    "    A_prev = A\n",
    "\n",
    "    A, cache = activation_part(A_prev, parameters[\"W\" + str(l)],parameters[\"b\" + str(l)], \"relu\")\n",
    "    caches.append(cache)\n",
    "\n",
    "\n",
    "  AL, cache = activation_part(A, parameters[\"W\" + str(l)],parameters[\"b\" + str(l)], \"sigmoid\")\n",
    "  caches.append(cache)\n",
    "\n",
    "\n",
    "  return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b58571c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the cost\n",
    "def compute_cost(AL, Y):\n",
    "\n",
    "  m = np.shape(Y)[0]\n",
    "\n",
    "\n",
    "  cost = (-1/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T))\n",
    "\n",
    "  cost = np.squeeze(cost)\n",
    "\n",
    "  return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a053e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "\n",
    "\n",
    "  A_prev, W, b = cache\n",
    "\n",
    "  m = A_prev.shape[0]\n",
    "\n",
    "  dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "  db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "  dA_prev = np.dot(W.T,dZ)\n",
    "\n",
    "\n",
    "\n",
    "  return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cfd4e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "\n",
    "    Z = cache\n",
    "    sigmoid_Z = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * sigmoid_Z * (1 - sigmoid_Z)\n",
    "\n",
    "    return dZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "14bea319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single ReLU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, same shape as A\n",
    "\n",
    "    Returns:\n",
    "    dZ -- gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    return dZ\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e2e331a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "\n",
    "  linear_cache, activation_cache = cache\n",
    "\n",
    "\n",
    "  if activation == \"relu\":\n",
    "    dZ = relu_backward(dA, activation_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "\n",
    "\n",
    "  elif activation == \"sigmoid\":\n",
    "\n",
    "    dZ = sigmoid_backward(dA, activation_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "\n",
    "\n",
    "  return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "653a11ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    current_cache = caches[L-1] # Last Layer\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e5293855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "\n",
    "    parameters = copy.deepcopy(params)\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(L):\n",
    "\n",
    "\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "56a9b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_x = 7     # num_px * num_px * 3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)\n",
    "learning_rate = 0.0075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7b0d5c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations\n",
    "\n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "\n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    #(â‰ˆ 1 line of code)\n",
    "    # parameters = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    parameters = initialize_parameters((n_x, n_h, n_y))\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
    "        #(â‰ˆ 2 lines of code)\n",
    "        # A1, cache1 = ...\n",
    "        # A2, cache2 = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        A1, cache1 = activation_part(X, W1, b1, \"relu\")\n",
    "        A2, cache2 = activation_part(A1, W2, b2, \"sigmoid\")\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        # Compute cost\n",
    "        #(â‰ˆ 1 line of code)\n",
    "        # cost = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        cost = compute_cost(A2, Y)\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "\n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        #(â‰ˆ 2 lines of code)\n",
    "        # dA1, dW2, db2 = ...\n",
    "        # dA0, dW1, db1 = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, \"relu\")\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "\n",
    "        # Update parameters.\n",
    "        #(approx. 1 line of code)\n",
    "        # parameters = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "\n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters, costs, A2\n",
    "\n",
    "def plot_costs(costs, learning_rate=0.0075):\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "08df3813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1109.1237077352948\n",
      "Cost after iteration 100: 1.6377219853135188\n",
      "Cost after iteration 200: 0.6921064097560468\n",
      "Cost after iteration 300: 0.43059841087398953\n",
      "Cost after iteration 400: 0.3104370200537869\n",
      "Cost after iteration 500: 0.24182932176205918\n",
      "Cost after iteration 600: 0.1975681804630608\n",
      "Cost after iteration 700: 0.1666957335720538\n",
      "Cost after iteration 800: 0.14396523020736993\n",
      "Cost after iteration 900: 0.12656285451466862\n",
      "Cost after iteration 999: 0.11294485810140925\n",
      "Cost after first iteration: 1109.1237077352948\n"
     ]
    }
   ],
   "source": [
    "parameters, costs, A2 = two_layer_model(X_train, y_train, layers_dims = (n_x, n_h, n_y), num_iterations = 1000, print_cost=True)\n",
    "\n",
    "print(\"Cost after first iteration: \" + str(costs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f99925b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AL, Caches = L_model_forward(X_test, parameters)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e666c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1, cache1 = activation_part(X_test, parameters[\"W1\"], parameters[\"b1\"], \"relu\")\n",
    "A2, cache2 = activation_part(A1, parameters[\"W2\"], parameters[\"b2\"], \"sigmoid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "83e91479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6400)\n",
      "[0 0 0 ... 0 0 0]\n",
      "6400\n"
     ]
    }
   ],
   "source": [
    "A2[0][0]\n",
    "print(np.shape(y_test))\n",
    "print(y_test[0])\n",
    "\n",
    "teller = 0\n",
    "\n",
    "for i in y_test[0]:\n",
    "    if i == 0:\n",
    "        teller += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(teller)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd468e64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
